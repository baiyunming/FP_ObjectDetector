{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pycocotools.coco as coco\n",
    "from pycocotools.cocoeval import COCOeval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load validation_annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=1.06s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "split = 'val'\n",
    "annot_path = os.path.join('F:\\TUM Learning Material\\Forschung\\CenterNet\\CenterNet-master\\data\\coco', 'annotations','instances_{}2017.json').format(split)\n",
    "#initialize coco object given file_path\n",
    "coco = coco.COCO(annot_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load detection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#coco_dets = coco.loadRes('{}/results.json'.format('F:\\TUM Learning Material\\Forschung\\CenterNet\\CenterNet-master\\exp\\ctdet\\hourglass'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preparing results...\n",
      "DONE (t=0.44s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "maskrcnn_dets = coco.loadRes('F:\\TUM Learning Material\\Forschung\\maskrcnn_detection_results.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sorted(maskrcnn_dets.getCatIds()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Params:\n",
    "    '''\n",
    "    Params for coco evaluation api\n",
    "    '''\n",
    "    def setDetParams(self):\n",
    "        self.imgIds = []\n",
    "        self.catIds = []\n",
    "        # np.arange causes trouble.  the data point on arange is slightly larger than the true value\n",
    "        self.iouThrs = np.linspace(.5, 0.95, int(np.round((0.95 - .5) / .05)) + 1, endpoint=True)\n",
    "        self.recThrs = np.linspace(.0, 1.00, int(np.round((1.00 - .0) / .01)) + 1, endpoint=True)\n",
    "        self.maxDets = [1, 10, 100]\n",
    "        self.areaRng = [[0 ** 2, 1e5 ** 2], [0 ** 2, 32 ** 2], [32 ** 2, 96 ** 2], [96 ** 2, 1e5 ** 2]]\n",
    "        self.areaRngLbl = ['all', 'small', 'medium', 'large']\n",
    "        self.useCats = 1\n",
    "\n",
    "\n",
    "    def __init__(self, iouType='segm'):\n",
    "        self.setDetParams()\n",
    "        self.iouType = iouType\n",
    "        # useSegm is deprecated\n",
    "        self.useSegm = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import datetime\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from pycocotools import mask as maskUtils\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cocoGt   = coco              # ground truth COCO API\n",
    "cocoDt   = yolo_dets              # detections COCO API\n",
    "evalImgs = defaultdict(list)   # per-image per-category evaluation results [KxAxI] elements\n",
    "eval     = {}                  # accumulated evaluation results\n",
    "_gts = defaultdict(list)       # gt for evaluation\n",
    "_dts = defaultdict(list)       # dt for evaluation\n",
    "params = Params(iouType='bbox') # parameters\n",
    "_paramsEval = {}               # parameters for evaluation\n",
    "stats = []                     # result summarization\n",
    "ious = {}                      # ious between all gts and dts\n",
    "if not cocoGt is None:\n",
    "    params.imgIds = sorted(cocoGt.getImgIds())\n",
    "    params.catIds = sorted(cocoGt.getCatIds())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(len(params.imgIds))\n",
    "print(len(params.catIds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "len(cocoGt.getAnnIds(imgIds=params.imgIds, catIds=params.catIds)) #all anotations are loaded "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "len(cocoDt.getAnnIds(imgIds=params.imgIds, catIds=params.catIds)) # for each image (50000) get 100 detections --> different maximum detection as hypyerparamter will be analyse "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gts=cocoGt.loadAnns(cocoGt.getAnnIds(imgIds=params.imgIds, catIds=params.catIds))\n",
    "dts=cocoDt.loadAnns(cocoDt.getAnnIds(imgIds=params.imgIds, catIds=params.catIds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dts[0] #example of gts --> purther processed for evaluation _gt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set ignore flag, ignore is not available in all annotations \n",
    "for gt in gts:\n",
    "    gt['ignore'] = gt['ignore'] if 'ignore' in gt else 0\n",
    "    gt['ignore'] = 'iscrowd' in gt and gt['iscrowd']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## new list for evaluation (_gts, _dts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _gts = defaultdict(list)       # gt for evaluation\n",
    "# _dts = defaultdict(list)       # dt for evaluation\n",
    "for gt in gts:\n",
    "    _gts[gt['image_id'], gt['category_id']].append(gt)\n",
    "for dt in dts:\n",
    "    _dts[dt['image_id'], dt['category_id']].append(dt)\n",
    "\n",
    "#evalImgs = defaultdict(list)   # per-image per-category evaluation results\n",
    "#eval     = {}                  # accumulated evaluation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#_gts.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgIds = list(np.unique(params.imgIds))\n",
    "if params.useCats:\n",
    "    catIds = list(np.unique(params.catIds))\n",
    "maxDets = sorted(params.maxDets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(imgIds))\n",
    "print(len(catIds))\n",
    "maxDets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## compute iou between gt and detection (corresponding to imgId and catId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeIoU(params, imgId, catId, _gts, _dts):\n",
    "    p = params\n",
    "    gt = _gts[imgId,catId]\n",
    "    dt = _dts[imgId,catId]\n",
    "\n",
    "    if len(gt) == 0 and len(dt) ==0:\n",
    "        return []\n",
    "    \n",
    "    inds = np.argsort([-d['score'] for d in dt], kind='mergesort')\n",
    "    #print(inds)\n",
    "    dt = [dt[i] for i in inds]\n",
    "    \n",
    "    if len(dt) > p.maxDets[-1]: #maximum 100 detection\n",
    "        dt=dt[0:p.maxDets[-1]] \n",
    "    \n",
    "    #extract bounding boxes \n",
    "    g = [g['bbox'] for g in gt]\n",
    "    d = [d['bbox'] for d in dt]\n",
    "\n",
    "    # compute iou between each dt and gt region\n",
    "    iscrowd = [int(o['iscrowd']) for o in gt]\n",
    "    ious = maskUtils.iou(d,g,iscrowd)\n",
    "    return ious"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ious = {(imgId, catId): computeIoU(params, imgId, catId, _gts, _dts) \\\n",
    "                        for imgId in imgIds\n",
    "                        for catId in catIds}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(_gts[(139, 64)])) #2 ground_truth bounding boxes \n",
    "print(len(_dts[(139, 64)])) #10 detections "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ious[(139,64)].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params.areaRng # all, small, middel. large "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## find matching between gt and detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateImg(params, ious, imgId, catId, aRng, maxDet):\n",
    "        '''\n",
    "        perform evaluation for single category and image\n",
    "        :return: dict (single image results)\n",
    "        '''\n",
    "        p = params\n",
    "        gt = _gts[imgId,catId]\n",
    "        dt = _dts[imgId,catId]\n",
    "\n",
    "        if len(gt) == 0 and len(dt) ==0:\n",
    "            return None\n",
    "        \n",
    "        #case 1: ground_truth bounding box already set to ignore \n",
    "        #case 2: bounding boxes out of evaluation range \n",
    "        for g in gt:\n",
    "            if g['ignore'] or (g['area']<aRng[0] or g['area']>aRng[1]):\n",
    "                g['_ignore'] = 1\n",
    "            else:\n",
    "                g['_ignore'] = 0\n",
    "\n",
    "        # sort dt highest score first, sort gt ignore last\n",
    "        gtind = np.argsort([g['_ignore'] for g in gt], kind='mergesort')\n",
    "        gt = [gt[i] for i in gtind]\n",
    "        dtind = np.argsort([-d['score'] for d in dt], kind='mergesort')\n",
    "        dt = [dt[i] for i in dtind[0:maxDet]]\n",
    "        iscrowd = [int(o['iscrowd']) for o in gt]\n",
    "        # load computed ious\n",
    "        ious = ious[imgId, catId][:, gtind] if len(ious[imgId, catId]) > 0 else ious[imgId, catId]\n",
    "\n",
    "        T = len(p.iouThrs)\n",
    "        G = len(gt)\n",
    "        D = len(dt)\n",
    "        gtm  = np.zeros((T,G))\n",
    "        dtm  = np.zeros((T,D))\n",
    "        gtIg = np.array([g['_ignore'] for g in gt])\n",
    "        dtIg = np.zeros((T,D))\n",
    "        if not len(ious)==0:\n",
    "            for tind, t in enumerate(p.iouThrs):\n",
    "                for dind, d in enumerate(dt):\n",
    "                    # information about best match so far (m=-1 -> unmatched)\n",
    "                    iou = min([t,1-1e-10])\n",
    "                    m   = -1\n",
    "                    for gind, g in enumerate(gt):\n",
    "                        # if this gt already matched, and not a crowd, continue\n",
    "                        if gtm[tind,gind]>0 and not iscrowd[gind]:\n",
    "                            continue\n",
    "                        # if dt matched to regular (not ignores) gt, and now on ignore gt, stop\n",
    "                        if m>-1 and gtIg[m]==0 and gtIg[gind]==1:\n",
    "                            break\n",
    "                        # continue to next gt unless better match made\n",
    "                        if ious[dind,gind] < iou:\n",
    "                            continue\n",
    "                        # if match successful and best so far, store appropriately\n",
    "                        iou=ious[dind,gind]\n",
    "                        m=gind\n",
    "                    # if match made store id of match for both dt and gt\n",
    "                    if m ==-1:\n",
    "                        continue\n",
    "                    dtIg[tind,dind] = gtIg[m]\n",
    "                    dtm[tind,dind]  = gt[m]['id']\n",
    "                    gtm[tind,m]     = d['id']\n",
    "        # set unmatched detections outside of area range to ignore\n",
    "        a = np.array([d['area']<aRng[0] or d['area']>aRng[1] for d in dt]).reshape((1, len(dt)))\n",
    "        dtIg = np.logical_or(dtIg, np.logical_and(dtm==0, np.repeat(a,T,0)))\n",
    "        # store results for given image and category\n",
    "        return {\n",
    "                'image_id':     imgId,\n",
    "                'category_id':  catId,\n",
    "                'aRng':         aRng,\n",
    "                'maxDet':       maxDet,\n",
    "                'dtIds':        [d['id'] for d in dt],\n",
    "                'gtIds':        [g['id'] for g in gt],\n",
    "                'dtMatches':    dtm,\n",
    "                'gtMatches':    gtm,\n",
    "                'dtScores':     [d['score'] for d in dt],\n",
    "                'gtIgnore':     gtIg,\n",
    "                'dtIgnore':     dtIg,\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxDet = params.maxDets[-1]\n",
    "evalImgs = [evaluateImg(params, ious, imgId, catId, areaRng, maxDet)\n",
    "                 for catId in catIds\n",
    "                 for areaRng in params.areaRng\n",
    "                 for imgId in params.imgIds\n",
    "           ]\n",
    "_paramsEval = copy.deepcopy(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "evalImgs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## accumulate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accumulate(params, _paramsEval, evalImgs):\n",
    "        '''\n",
    "        Accumulate per image evaluation results and store the result in self.eval\n",
    "        :param p: input params for evaluation\n",
    "        :return: None\n",
    "        '''\n",
    "        print('Accumulating evaluation results...')\n",
    "        # allows input customized parameters\n",
    "        p = params\n",
    "        p.catIds = p.catIds if p.useCats == 1 else [-1]\n",
    "        T           = len(p.iouThrs)\n",
    "        R           = len(p.recThrs)\n",
    "        K           = len(p.catIds) if p.useCats else 1\n",
    "        A           = len(p.areaRng)\n",
    "        M           = len(p.maxDets)\n",
    "        precision   = -np.ones((T,R,K,A,M)) # -1 for the precision of absent categories\n",
    "        recall      = -np.ones((T,K,A,M))\n",
    "        scores      = -np.ones((T,R,K,A,M))\n",
    "\n",
    "        # create dictionary for future indexing\n",
    "        _pe = _paramsEval\n",
    "        catIds = _pe.catIds if _pe.useCats else [-1]\n",
    "        setK = set(catIds)\n",
    "        setA = set(map(tuple, _pe.areaRng))\n",
    "        setM = set(_pe.maxDets)\n",
    "        setI = set(_pe.imgIds)\n",
    "        # get inds to evaluate\n",
    "        k_list = [n for n, k in enumerate(p.catIds)  if k in setK] #category\n",
    "        m_list = [m for n, m in enumerate(p.maxDets) if m in setM] #maximum_Detections \n",
    "        a_list = [n for n, a in enumerate(map(lambda x: tuple(x), p.areaRng)) if a in setA] #4 areaRanges: total, small, middle, large \n",
    "        i_list = [n for n, i in enumerate(p.imgIds)  if i in setI] #all_images 5000\n",
    "        I0 = len(_pe.imgIds)\n",
    "        A0 = len(_pe.areaRng)\n",
    "        # retrieve E at each category, area range, and max number of detections\n",
    "        for k, k0 in enumerate(k_list):  #category\n",
    "            Nk = k0*A0*I0\n",
    "            for a, a0 in enumerate(a_list): #4 areaRanges: total, small, middle, large \n",
    "                Na = a0*I0\n",
    "                for m, maxDet in enumerate(m_list): #all_images 5000\n",
    "                    E = [evalImgs[Nk + Na + i] for i in i_list]\n",
    "                    E = [e for e in E if not e is None]\n",
    "                    if len(E) == 0:\n",
    "                        continue\n",
    "                    \n",
    "                    #same area range, max number of detections --> evaluate all 5000 val images  \n",
    "                    dtScores = np.concatenate([e['dtScores'][0:maxDet] for e in E])\n",
    "\n",
    "                    # different sorting method generates slightly different results.\n",
    "                    inds = np.argsort(-dtScores, kind='mergesort')\n",
    "                    dtScoresSorted = dtScores[inds]\n",
    "                    \n",
    "                    #detection matching matrix \n",
    "                    dtm  = np.concatenate([e['dtMatches'][:,0:maxDet] for e in E], axis=1)[:,inds]\n",
    "                    #detection ignore matrix \n",
    "                    dtIg = np.concatenate([e['dtIgnore'][:,0:maxDet]  for e in E], axis=1)[:,inds]\n",
    "                    #ground_truth ignore matrix \n",
    "                    gtIg = np.concatenate([e['gtIgnore'] for e in E])\n",
    "                    #calculate recall \n",
    "                    npig = np.count_nonzero(gtIg==0 )\n",
    "                    \n",
    "                    if npig == 0:\n",
    "                        continue\n",
    "                    \n",
    "                    #true positive and false positive \n",
    "                    #detection but not matched to any ground_truth --> false positive \n",
    "                    tps = np.logical_and(               dtm,  np.logical_not(dtIg) )\n",
    "                    fps = np.logical_and(np.logical_not(dtm), np.logical_not(dtIg) )\n",
    "\n",
    "                    tp_sum = np.cumsum(tps, axis=1).astype(dtype=np.float)\n",
    "                    fp_sum = np.cumsum(fps, axis=1).astype(dtype=np.float)\n",
    "                    for t, (tp, fp) in enumerate(zip(tp_sum, fp_sum)): #extract each row --> correspond to different iouThresholds  \n",
    "                        tp = np.array(tp) \n",
    "                        fp = np.array(fp)\n",
    "                        nd = len(tp)\n",
    "                        rc = tp / npig\n",
    "                        pr = tp / (fp+tp+np.spacing(1)) #true positve/ all detections \n",
    "                        q  = np.zeros((R,))\n",
    "                        ss = np.zeros((R,))\n",
    "\n",
    "                        if nd:\n",
    "                            recall[t,k,a,m] = rc[-1]\n",
    "                        else:\n",
    "                            recall[t,k,a,m] = 0\n",
    "\n",
    "                        # numpy is slow without cython optimization for accessing elements\n",
    "                        # use python array gets significant speed improvement\n",
    "                        pr = pr.tolist(); q = q.tolist()\n",
    "\n",
    "                        for i in range(nd-1, 0, -1):\n",
    "                            if pr[i] > pr[i-1]:\n",
    "                                pr[i-1] = pr[i]\n",
    "\n",
    "                        inds = np.searchsorted(rc, p.recThrs, side='left')\n",
    "                        try:\n",
    "                            for ri, pi in enumerate(inds):\n",
    "                                q[ri] = pr[pi]\n",
    "                                ss[ri] = dtScoresSorted[pi]\n",
    "                        except:\n",
    "                            pass\n",
    "                        precision[t,:,k,a,m] = np.array(q)\n",
    "                        scores[t,:,k,a,m] = np.array(ss)\n",
    "        return {\n",
    "            'params': p,\n",
    "            'counts': [T, R, K, A, M],\n",
    "            'precision': precision,\n",
    "            'recall':   recall,\n",
    "            'scores': scores,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params.recThrs #recall thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval = accumulate(params, _paramsEval, evalImgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval['precision'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval['recall'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _summarize( eval, ap=1, iouThr=None, areaRng='all', maxDets=100 ):\n",
    "        p = self.params\n",
    "        iStr = ' {:<18} {} @[ IoU={:<9} | area={:>6s} | maxDets={:>3d} ] = {:0.3f}'\n",
    "        titleStr = 'Average Precision' if ap == 1 else 'Average Recall'\n",
    "        typeStr = '(AP)' if ap==1 else '(AR)'\n",
    "        iouStr = '{:0.2f}:{:0.2f}'.format(p.iouThrs[0], p.iouThrs[-1]) \\\n",
    "            if iouThr is None else '{:0.2f}'.format(iouThr)\n",
    "\n",
    "        aind = [i for i, aRng in enumerate(p.areaRngLbl) if aRng == areaRng]\n",
    "        mind = [i for i, mDet in enumerate(p.maxDets) if mDet == maxDets]\n",
    "        \n",
    "        if ap == 1:\n",
    "            # dimension of precision: [TxRxKxAxM]\n",
    "            s = eval['precision']\n",
    "            # IoU\n",
    "            if iouThr is not None:\n",
    "                t = np.where(iouThr == p.iouThrs)[0]\n",
    "                s = s[t]\n",
    "            s = s[:,:,:,aind,mind]\n",
    "        else:\n",
    "            # dimension of recall: [TxKxAxM]\n",
    "            s = eval['recall']\n",
    "            if iouThr is not None:\n",
    "                t = np.where(iouThr == p.iouThrs)[0]\n",
    "                s = s[t]\n",
    "            s = s[:,:,aind,mind]\n",
    "        if len(s[s>-1])==0:\n",
    "            mean_s = -1\n",
    "        else:\n",
    "            mean_s = np.mean(s[s>-1])\n",
    "        print(iStr.format(titleStr, typeStr, iouStr, areaRng, maxDets, mean_s))\n",
    "        return mean_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aind = [i for i, aRng in enumerate(params.areaRngLbl) if aRng == 'all'] #area_range index \n",
    "mind = [i for i, mDet in enumerate(params.maxDets) if mDet == 10] #mDetection index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_matrix = eval['precision']\n",
    "s = precision_matrix[:,:,:,aind,mind]\n",
    "precision_result = s[s>-1].reshape(10,101,80)\n",
    "mean_s = np.mean(precision_result, axis=0)\n",
    "category_result = np.mean(mean_s, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_sort_idx = np.argsort(-category_result, kind='mergesort')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_sort_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_name = ['person', 'bicycle', 'car', 'motorcycle', 'airplane','bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant',\n",
    "      'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse',\n",
    "      'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack',\n",
    "      'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis',\n",
    "      'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove',\n",
    "      'skateboard', 'surfboard', 'tennis racket', 'bottle', 'wine glass',\n",
    "      'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich',\n",
    "      'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake',\n",
    "      'chair', 'couch', 'potted plant', 'bed', 'dining table', 'toilet', 'tv',\n",
    "      'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave',\n",
    "      'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase',\n",
    "      'scissors', 'teddy bear', 'hair drier', 'toothbrush']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for i in range(len(precision_sort_idx)):\n",
    "    print(class_name[precision_sort_idx[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_name[precision_sort_idx[0]-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(category_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_matrix = eval['recall']\n",
    "recall = recall_matrix[:,:,aind,mind]\n",
    "recall_result = recall[recall>-1].reshape(10,80)\n",
    "mean_recall = np.mean(recall_result, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_sort_idx = np.argsort(-mean_recall, kind='mergesort')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_sort_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for i in range(len(recall_sort_idx)):\n",
    "    print(class_name[recall_sort_idx[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:centernet]",
   "language": "python",
   "name": "conda-env-centernet-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
